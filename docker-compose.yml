---
version: '3.9'

services:
  main-llm-responder:
    build:
      context: services/llm-endpoint/
      dockerfile: llm-endpoint.Dockerfile
      args:
        EXPOSED_PORT: 11434

    container_name: main-llm-responder
    ports:
      - 11434:11434
    volumes:
      - ./persist_dir/main-llm-responder:/root/.ollama
    networks:
      - rag-net
    environment:
      - LLM_GENERATOR_MODEL=llama3.1:8b
      - OLLAMA_DEBUG=0
      - OLLAMA_HOST=0.0.0.0:11434

  guard-llm:
    build:
      context: services/llm-endpoint/
      dockerfile: llm-endpoint.Dockerfile
      args:
        EXPOSED_PORT: 11435

    container_name: guard-llm
    ports:
      - 11435:11435
    volumes:
      - ./persist_dir/guard-llm:/root/.ollama
    networks:
      - rag-net
    environment:
      - LLM_GENERATOR_MODEL=dolphin3:8b
      - OLLAMA_DEBUG=0
      - OLLAMA_HOST=0.0.0.0:11435
    
  vector-store:
    build:
      context: services/vector-store/
      dockerfile: vector-store.Dockerfile
      args:
        EXPOSED_PORT_REST: 6333
        EXPOSED_PORT_GRPC: 6334

    container_name: vector-store
    ports:
      - 6333:6333
      - 6334:6334
    volumes:
      - ./persist_dir/vector-store:/qdrant/storage:z
    networks:
      - rag-net

  embedding-endpoint:
    build:
      context: services/embedding-endpoint/
      dockerfile: embedding-endpoint.Dockerfile
      args:
        EXPOSED_PORT: 11436
    
    container_name: embedding-endpoint
    ports:
      - 11436:11436
    volumes:
      - ./persist_dir/embedding-endpoint:/root/.ollama
    networks:
      - rag-net
    environment:
      - EMBEDDING_MODEL=nomic-embed-text:v1.5
      - OLLAMA_DEBUG=0
      - OLLAMA_HOST=0.0.0.0:11436

networks:
  rag-net:
    driver: bridge
